{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part A: Data Setup"
      ],
      "metadata": {
        "id": "kDuCn3a_PSBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# A1. Load dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
        "column_names = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole_weight\",\n",
        "                \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\"]\n",
        "df = pd.read_csv(url, names=column_names)\n",
        "\n",
        "print(f\"Number of rows: {len(df)}\")\n",
        "print(f\"Column names: {list(df.columns)}\")\n",
        "print(\"First 5 rows:\\n\", df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvaybS3xOTwj",
        "outputId": "d630d876-f95a-4668-d663-d258989ffaaa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 4177\n",
            "Column names: ['Sex', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight', 'Rings']\n",
            "First 5 rows:\n",
            "   Sex  Length  Diameter  Height  Whole_weight  Shucked_weight  Viscera_weight  \\\n",
            "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
            "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
            "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
            "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
            "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
            "\n",
            "   Shell_weight  Rings  \n",
            "0         0.150     15  \n",
            "1         0.070      7  \n",
            "2         0.210      9  \n",
            "3         0.155     10  \n",
            "4         0.055      7  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoint A1:\n",
        "# What is input: The physical measurements of the abalone (e.g., Length, Diameter, Weight).\n",
        "# What is output: The age of the abalone (derived from Rings).\n",
        "# Why output is numeric: Age is a continuous, measurable quantity, making this a regression problem."
      ],
      "metadata": {
        "id": "t4J3lZYvOnDK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A2. Convert target\n",
        "y = df['Rings'].values + 1.5"
      ],
      "metadata": {
        "id": "3Y-HHQr2OxbL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A3. Choose exactly 3 numeric features\n",
        "X_raw = df[['Length', 'Diameter', 'Whole_weight']].values"
      ],
      "metadata": {
        "id": "Vwot1jPoOz2T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Justification:\n",
        "# Feature 1 (Length): The longest shell measurement directly relates to the organism's growth duration.\n",
        "# Feature 2 (Diameter): Perpendicular to length, providing a 2D understanding of size.\n",
        "# Feature 3 (Whole_weight): Overall mass is a strong indicator of how long the abalone has been growing and feeding."
      ],
      "metadata": {
        "id": "yTu-BcO4O5ji"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A4. Train-test split (80/20 without sklearn)\n",
        "np.random.seed(42)\n",
        "indices = np.random.permutation(len(X_raw))\n",
        "split_idx = int(0.8 * len(X_raw))\n",
        "\n",
        "train_idx, test_idx = indices[:split_idx], indices[split_idx:]\n",
        "X_train_raw, X_test_raw = X_raw[train_idx], X_raw[test_idx]\n",
        "y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "print(f\"X_train shape: {X_train_raw.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test_raw.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_gtkwYCO9wS",
        "outputId": "3d405517-fbc9-47c7-87d1-dfac0687e061"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (3341, 3), y_train shape: (3341, 1)\n",
            "X_test shape: (836, 3), y_test shape: (836, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A5. Normalize inputs\n",
        "train_mean = np.mean(X_train_raw, axis=0)\n",
        "train_std = np.std(X_train_raw, axis=0)\n",
        "\n",
        "X_train = (X_train_raw - train_mean) / train_std\n",
        "X_test = (X_test_raw - train_mean) / train_std\n",
        "\n",
        "# Checkpoint A5:\n",
        "# why normalization is needed for learning: It ensures all features are on the same scale, preventing features with larger numeric ranges from dominating the gradient updates, and helps gradient descent converge much faster."
      ],
      "metadata": {
        "id": "LMyoKll9PKYq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B: Define the model"
      ],
      "metadata": {
        "id": "0TrstBKaPWsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# B. Define the model\n",
        "def forward(X, w, b):\n",
        "    return np.dot(X, w) + b\n",
        "\n",
        "# Initialize dummy parameters to print shapes once as required\n",
        "d = X_train.shape[1]\n",
        "w_dummy = np.zeros((d, 1))\n",
        "b_dummy = 0.0\n",
        "y_hat_dummy = forward(X_train, w_dummy, b_dummy)\n",
        "\n",
        "print(f\"Shape of X: {X_train.shape}\")\n",
        "print(f\"Shape of w: {w_dummy.shape}\")\n",
        "print(f\"Shape of b: scalar\")\n",
        "print(f\"Shape of y_hat: {y_hat_dummy.shape}\")\n",
        "\n",
        "# Checkpoint B:\n",
        "# parameters are: w_1, w_2, w_3 (the weights) and b (the bias).\n",
        "# number of parameters: 4 total parameters for d=3 (3 weights + 1 bias)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUYIru-VPYTy",
        "outputId": "fde65c37-3336-4474-bac2-1c80b189fa52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (3341, 3)\n",
            "Shape of w: (3, 1)\n",
            "Shape of b: scalar\n",
            "Shape of y_hat: (3341, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part C: Define Loss (MSE)"
      ],
      "metadata": {
        "id": "saVj3IhOPlk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C. Define Loss\n",
        "def mse(y, y_hat):\n",
        "    return np.mean((y - y_hat) ** 2)\n",
        "\n",
        "# Checkpoint C:\n",
        "# why square: Squaring ensures negative and positive errors don't cancel each other out, and makes the loss function differentiable (a smooth bowl shape).\n",
        "# what mistakes are expensive: Large mistakes become exponentially more expensive due to the square, forcing the model to severely penalize outliers."
      ],
      "metadata": {
        "id": "LOoMdoI5PuzC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part D: The Learning Rule (Gradients)"
      ],
      "metadata": {
        "id": "G5O8G3UdPyka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# D. Implement gradients\n",
        "def grad_w(X, y, y_hat):\n",
        "    N = len(y)\n",
        "    dW = (-2/N) * np.dot(X.T, (y - y_hat))\n",
        "    return dW\n",
        "\n",
        "def grad_b(y, y_hat):\n",
        "    N = len(y)\n",
        "    db = (-2/N) * np.sum(y - y_hat)\n",
        "    return db\n",
        "\n",
        "# Checkpoint D:\n",
        "# what gradient means in words: The gradient points in the direction of the steepest slope (highest increase) of the loss function.\n",
        "# why subtracting gradient reduces loss: Since the gradient points UP the slope, subtracting it moves us down the slope towards the minimum error.\n",
        "# meaning of large gradient: A large gradient means the model's current predictions are very wrong, and it is on a steep part of the loss curve.\n",
        "# effect of too-large learning rate: The updates will be too massive, causing the model to step over the minimum and potentially diverge (loss explodes to infinity)."
      ],
      "metadata": {
        "id": "5VkzAoYgP3vS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part E: Training Loop"
      ],
      "metadata": {
        "id": "FThIClI4QAwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E. Training Loop\n",
        "np.random.seed(42)\n",
        "w = np.random.randn(d, 1) * 0.01\n",
        "b = 0.0\n",
        "lr = 0.1\n",
        "epochs = 200\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    y_hat = forward(X_train, w, b)\n",
        "    loss = mse(y_train, y_hat)\n",
        "    dW = grad_w(X_train, y_train, y_hat)\n",
        "    db = grad_b(y_train, y_hat)\n",
        "    w = w - lr * dW\n",
        "    b = b - lr * db\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Checkpoint E:\n",
        "# Initial expectation: The loss should go down rapidly at first because the random initial weights will produce massive errors, yielding large gradients.\n",
        "# Revised expectation after training: It did drop fast initially, but then plateaued as it approached the minimum, slowing down because the gradients become very small near the bottom of the error curve."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0JDEW_nQFdK",
        "outputId": "54b7a2f5-e2b0-4e6c-8585-cde0ce2362a9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 141.4974\n",
            "Epoch 20, Loss: 7.2441\n",
            "Epoch 40, Loss: 7.2129\n",
            "Epoch 60, Loss: 7.2010\n",
            "Epoch 80, Loss: 7.1905\n",
            "Epoch 100, Loss: 7.1811\n",
            "Epoch 120, Loss: 7.1728\n",
            "Epoch 140, Loss: 7.1654\n",
            "Epoch 160, Loss: 7.1587\n",
            "Epoch 180, Loss: 7.1527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part F: Evaluation"
      ],
      "metadata": {
        "id": "_pACqKNOQT9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_hat = forward(X_test, w, b)\n",
        "test_mse = mse(y_test, y_test_hat)\n",
        "test_mae = np.mean(np.abs(y_test - y_test_hat))\n",
        "\n",
        "print(f\"\\nTest MSE: {test_mse:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "\n",
        "print(\"5 Example Predictions:\")\n",
        "for i in range(5):\n",
        "    true_age = y_test[i][0]\n",
        "    pred_age = y_test_hat[i][0]\n",
        "    abs_err = abs(true_age - pred_age)\n",
        "    print(f\"Example {i+1}: True Age = {true_age:.2f} | Predicted = {pred_age:.2f} | Abs Error = {abs_err:.2f}\")\n",
        "\n",
        "# Checkpoint F:\n",
        "# systematic errors: The model likely systematically underestimates the age of very old abalones, because linear regressions struggle with extreme non-linear tails.\n",
        "# observed bias: There is a base prediction \"bias\" heavily anchored around the dataset's average age, struggling to perfectly capture variance just using 3 basic physical traits."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-rRZdUDQXyx",
        "outputId": "4f639014-2dd2-419b-ddab-163b3a8f0ebd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test MSE: 6.1823\n",
            "Test MAE: 1.8641\n",
            "5 Example Predictions:\n",
            "Example 1: True Age = 10.50 | Predicted = 9.99 | Abs Error = 0.51\n",
            "Example 2: True Age = 11.50 | Predicted = 13.69 | Abs Error = 2.19\n",
            "Example 3: True Age = 10.50 | Predicted = 12.49 | Abs Error = 1.99\n",
            "Example 4: True Age = 11.50 | Predicted = 9.22 | Abs Error = 2.28\n",
            "Example 5: True Age = 7.50 | Predicted = 8.41 | Abs Error = 0.91\n"
          ]
        }
      ]
    }
  ]
}