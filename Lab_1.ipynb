{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I120bpSk6nXI"
      },
      "outputs": [],
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, n, lr=0.1):\n",
        "        self.lr = lr\n",
        "        self.wts = [0] * n\n",
        "        self.b = 0\n",
        "\n",
        "    def activation(self, z):\n",
        "        return 1 if z >= 0 else 0\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        z = sum(w * x for w, x in zip(self.wts, inputs)) + self.b\n",
        "        return self.activation(z)\n",
        "\n",
        "    def train(self, inputs, labels, epochs=20):\n",
        "        for epoch in range(epochs):\n",
        "            for x, target in zip(inputs, labels):\n",
        "                prediction = self.predict(x)\n",
        "                error = target - prediction\n",
        "                for i in range(len(self.wts)):\n",
        "                    self.wts[i] += self.lr * error * x[i]\n",
        "                self.b += self.lr * error\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "\n",
        "gates = {\n",
        "    \"AND\":  [0, 0, 0, 1],\n",
        "    \"OR\":   [0, 1, 1, 1],\n",
        "    \"NAND\": [1, 1, 1, 0],\n",
        "    \"NOR\":  [1, 0, 0, 0],\n",
        "    \"XOR\":  [0, 1, 1, 0]\n",
        "}\n",
        "\n",
        "print(f\"{'GATE':<5} | {'Final Weights':<15} | {'Bias':<5} | {'Correct?'}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for gate_name, target in gates.items():\n",
        "    p = Perceptron(2)\n",
        "    p.train(X, target, epochs=50)\n",
        "\n",
        "    predictions = [p.predict(i) for i in X]\n",
        "    is_correct = \"PASS\" if predictions == target else \"FAIL\"\n",
        "\n",
        "    w_str = f\"[{p.wts[0]:.1f}, {p.wts[1]:.1f}]\"\n",
        "    print(f\"{gate_name:<5} | {w_str:<15} | {p.b:<5.1f} | {is_correct}\")"
      ],
      "metadata": {
        "id": "iep_ZQjN80mh",
        "outputId": "5a373a38-8f67-4ec6-9011-d4a3ce48b95b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GATE  | Final Weights   | Bias  | Correct?\n",
            "--------------------------------------------------\n",
            "AND   | [0.2, 0.1]      | -0.2  | PASS\n",
            "OR    | [0.1, 0.1]      | -0.1  | PASS\n",
            "NAND  | [-0.2, -0.1]    | 0.2   | PASS\n",
            "NOR   | [-0.1, -0.1]    | 0.0   | PASS\n",
            "XOR   | [-0.1, 0.0]     | 0.0   | FAIL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Effect of Learning Rate\n",
        "\n",
        "The learning rate dictates the step size the perceptron takes when updating its weights.\n",
        "\n",
        "- If it's too high: The weights will change too drastically. The perceptron might overshoot the optimal solution and fail to converge (bouncing back and forth around the correct answer).\n",
        "\n",
        "- If it's too low: The perceptron will learn very slowly, taking tiny steps. It will eventually find the right answer (for linearly separable data), but it will require many more epochs to get there."
      ],
      "metadata": {
        "id": "8zHhOjvM3wrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why the same code works for different Gates\n",
        "\n",
        "The perceptron code acts as a blank template, it's just an algorithm for moving a straight line (the decision boundary) around a graph based on feedback. The code doesn't know what an AND gate or an OR gate is. The perceptron learned different gates because we fed it different target labels. During training, the error calculation (error = target - prediction) pulls the weights in whatever direction is necessary to match the specific dataset we provided. The data shapes the model."
      ],
      "metadata": {
        "id": "Vqg72EJm4SI7"
      }
    }
  ]
}