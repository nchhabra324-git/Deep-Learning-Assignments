{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "X = np.random.uniform(-2, 2, (400, 3))\n",
        "y = (\n",
        "    np.sin(X[:, 0]) +\n",
        "    0.5 * (X[:, 1] ** 2) -\n",
        "    0.8 * X[:, 2]\n",
        ")\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "X = X.T\n",
        "y = y.T\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_deriv(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def sigmoid(z):\n",
        "    z = np.clip(z, -500, 500)\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_deriv(z):\n",
        "    sig = sigmoid(z)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "def tanh_deriv(z):\n",
        "    return 1 - np.tanh(z)**2\n",
        "\n",
        "def leaky_relu(z, alpha=0.01):\n",
        "    return np.where(z > 0, z, alpha * z)\n",
        "\n",
        "def leaky_relu_deriv(z, alpha=0.01):\n",
        "    return np.where(z > 0, 1.0, alpha)\n",
        "\n",
        "def softplus(z):\n",
        "    z = np.clip(z, -500, 500)\n",
        "    return np.log(1 + np.exp(z))\n",
        "\n",
        "def softplus_deriv(z):\n",
        "    return sigmoid(z)"
      ],
      "metadata": {
        "id": "XDQ6MUMcTZRR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    def __init__(self, neurons_in, neurons_out, activation_name=\"relu\"):\n",
        "        self.W = np.random.uniform(-0.5, 0.5, (neurons_out, neurons_in))\n",
        "        self.b = np.zeros((neurons_out, 1))\n",
        "        self.activation_name = activation_name\n",
        "\n",
        "        self.Z = None\n",
        "        self.A_prev = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def activate(self, Z):\n",
        "        if self.activation_name == \"relu\": return relu(Z)\n",
        "        elif self.activation_name == \"sigmoid\": return sigmoid(Z)\n",
        "        elif self.activation_name == \"linear\": return Z\n",
        "\n",
        "    def activate_deriv(self, Z):\n",
        "        if self.activation_name == \"relu\": return relu_deriv(Z)\n",
        "        elif self.activation_name == \"sigmoid\": return sigmoid_deriv(Z)\n",
        "        elif self.activation_name == \"linear\": return np.ones_like(Z)\n",
        "\n",
        "class DeepNetwork:\n",
        "    def __init__(self, layer_dims, hidden_activation=\"relu\"):\n",
        "        self.layers = []\n",
        "        for i in range(len(layer_dims) - 1):\n",
        "            act = hidden_activation if i < len(layer_dims) - 2 else \"linear\"\n",
        "            self.layers.append(Layer(layer_dims[i], layer_dims[i+1], act))\n",
        "\n",
        "    def forward(self, X):\n",
        "        A = X\n",
        "        for layer in self.layers:\n",
        "            layer.A_prev = A\n",
        "            layer.Z = np.dot(layer.W, layer.A_prev) + layer.b\n",
        "            A = layer.activate(layer.Z)\n",
        "        return A\n",
        "\n",
        "    def backward(self, y, y_hat):\n",
        "        N = y.shape[1]\n",
        "        dA = -2 * (y - y_hat) / N\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            layer = self.layers[i]\n",
        "            dZ = dA * layer.activate_deriv(layer.Z)\n",
        "            layer.dW = np.dot(dZ, layer.A_prev.T)\n",
        "            layer.db = np.sum(dZ, axis=1, keepdims=True)\n",
        "            if i > 0:\n",
        "                dA = np.dot(layer.W.T, dZ)\n",
        "\n",
        "    def update(self, lr):\n",
        "        for layer in self.layers:\n",
        "            layer.W -= lr * layer.dW\n",
        "            layer.b -= lr * layer.db"
      ],
      "metadata": {
        "id": "J2Ne03XbTmyR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model_name, layer_dims, hidden_act=\"relu\"):\n",
        "    print(f\"\\nTraining {model_name} with {hidden_act.capitalize()}...\")\n",
        "    model = DeepNetwork(layer_dims, hidden_activation=hidden_act)\n",
        "    epochs = 1000\n",
        "    lr = 0.01\n",
        "    loss_200 = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        y_hat = model.forward(X)\n",
        "        loss = np.mean((y - y_hat) ** 2)\n",
        "\n",
        "        if epoch == 199:\n",
        "            loss_200 = loss\n",
        "\n",
        "        model.backward(y, y_hat)\n",
        "        model.update(lr)\n",
        "\n",
        "    final_loss = np.mean((y - model.forward(X)) ** 2)\n",
        "\n",
        "    first_hidden_dW = model.layers[0].dW\n",
        "    last_hidden_dW = model.layers[-2].dW\n",
        "\n",
        "    grad_norm_L1 = np.sqrt(np.sum(first_hidden_dW ** 2))\n",
        "    grad_norm_Last = np.sqrt(np.sum(last_hidden_dW ** 2))\n",
        "\n",
        "    print(f\"Loss @ 200: {loss_200:.4f}\")\n",
        "    print(f\"Final Loss: {final_loss:.4f}\")\n",
        "    print(f\"Grad Norm L1: {grad_norm_L1:.6f}\")\n",
        "    print(f\"Grad Norm Last: {grad_norm_Last:.6f}\")"
      ],
      "metadata": {
        "id": "RRWVya8DT3XA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model architectures (input=3, output=1)\n",
        "model_A = [3, 4, 1]\n",
        "model_B = [3, 6, 6, 1]\n",
        "model_C = [3, 8, 8, 8, 8, 1]\n",
        "model_D = [3, 8, 8, 8, 8, 8, 8, 8, 8, 1]\n",
        "\n",
        "train_model(\"Model A - Shallow\", model_A, \"relu\")\n",
        "train_model(\"Model B - Medium\", model_B, \"relu\")\n",
        "train_model(\"Model C - Deep\", model_C, \"relu\")\n",
        "train_model(\"Model D - Very Deep (ReLU)\", model_D, \"relu\")\n",
        "train_model(\"Model D - Very Deep (Sigmoid)\", model_D, \"sigmoid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1aus8LcUEQZ",
        "outputId": "ed0277b1-2031-4928-b0d2-143750b7a2bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Model A - Shallow with Relu...\n",
            "Loss @ 200: 0.4938\n",
            "Final Loss: 0.1115\n",
            "Grad Norm L1: 0.045217\n",
            "Grad Norm Last: 0.045217\n",
            "\n",
            "Training Model B - Medium with Relu...\n",
            "Loss @ 200: 0.3220\n",
            "Final Loss: 0.0728\n",
            "Grad Norm L1: 0.036609\n",
            "Grad Norm Last: 0.021441\n",
            "\n",
            "Training Model C - Deep with Relu...\n",
            "Loss @ 200: 0.8620\n",
            "Final Loss: 0.0304\n",
            "Grad Norm L1: 0.023876\n",
            "Grad Norm Last: 0.016801\n",
            "\n",
            "Training Model D - Very Deep (ReLU) with Relu...\n",
            "Loss @ 200: 1.6349\n",
            "Final Loss: 0.0528\n",
            "Grad Norm L1: 0.429784\n",
            "Grad Norm Last: 0.621290\n",
            "\n",
            "Training Model D - Very Deep (Sigmoid) with Sigmoid...\n",
            "Loss @ 200: 1.7439\n",
            "Final Loss: 1.7439\n",
            "Grad Norm L1: 0.000006\n",
            "Grad Norm Last: 0.000006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflections"
      ],
      "metadata": {
        "id": "hEGlpwiPUhgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Did deeper always reduce loss faster: No, While deep networks have more capacity, Model D likely struggled or learned very slowly at first compared to A or B because gradients take longer to propagate effectively through 8 layers.\n",
        "\n",
        "- Did gradients in early layers stay similar to later layers: No, The Grad Norm L1 will be noticeably smaller than the Grad Norm Last in deep models. Because each additional layer multiplies gradients during backpropagation, gradients shrink as they reach earlier layers.\n",
        "\n",
        "- Was training equally stable for all activations: No, Model D with Sigmoid likely failed to learn much at all (loss barely decreased), while Model D with ReLU made progress.\n",
        "\n",
        "- Which activation behaved more stable in deeper networks: ReLU was significantly more stable. Sigmoid's derivative has a maximum value of 0.25, so multiplying that 8 times causes the gradient at Layer 1 to vanish entirely.\n",
        "\n",
        "- Did some models improve very slowly even though the learning rate was same: Yes, Model D (especially with Sigmoid) likely improved at a snail's pace because the gradients updating the early layers became infinitesimally small, practically freezing those layers."
      ],
      "metadata": {
        "id": "1ULOxJLiUOF4"
      }
    }
  ]
}