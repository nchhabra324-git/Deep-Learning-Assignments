{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m2raft-sVWSj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Task 1.1 - Input generation\n",
        "# X represents a single feature (e.g., time, distance) measured from 0 to 10.\n",
        "# This range ensures we see both early rapid change and later saturation, which a linear model cannot fit well.\n",
        "X = np.linspace(0, 10, 50).reshape(-1, 1)\n",
        "\n",
        "# Task 1.2 - Target generation\n",
        "# The logarithmic shape grows fast initially and slows later.\n",
        "# A straight line has only one slope and cannot match both regions simultaneously.\n",
        "noise = np.random.normal(0, 0.2, size=(50, 1))\n",
        "y = np.log(X + 1) + noise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many hidden units: 3.\n",
        "# Why more than 1: Hidden units allow multiple pieces or angles of behavior.\n",
        "# Why not too many: Too many units increase complexity and instability and too few cannot bend enough."
      ],
      "metadata": {
        "id": "JstwnTKWdHYG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "W1 = np.random.uniform(-1, 1, size=(1, 3))\n",
        "b1 = np.zeros((1, 3))\n",
        "\n",
        "W2 = np.random.uniform(-1, 1, size=(3, 1))\n",
        "b2 = np.zeros((1, 1))"
      ],
      "metadata": {
        "id": "3VL2s3D8dSLe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def activation(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def activation_slope(z):\n",
        "    return (z > 0).astype(float)"
      ],
      "metadata": {
        "id": "Oq-uI4_MdWF9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2000\n",
        "learning_rate = 0.01\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    z1 = X @ W1 + b1\n",
        "    h = activation(z1)\n",
        "    y_hat = h @ W2 + b2\n",
        "    error = y_hat - y\n",
        "    loss = np.mean(error ** 2)\n",
        "    dL_dy = 2 * error / len(X)\n",
        "    dL_dW2 = h.T @ dL_dy\n",
        "    dL_db2 = np.sum(dL_dy, axis=0, keepdims=True)\n",
        "    dL_dh = dL_dy @ W2.T\n",
        "    dL_dz1 = dL_dh * activation_slope(z1)\n",
        "    dL_dW1 = X.T @ dL_dz1\n",
        "    dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
        "    W1 -= learning_rate * dL_dW1\n",
        "    b1 -= learning_rate * dL_db1\n",
        "    W2 -= learning_rate * dL_dW2\n",
        "    b2 -= learning_rate * dL_db2\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss: {loss:.4f}\")\n",
        "\n",
        "print(f\"Final Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0FoUHq0dbP-",
        "outputId": "d6553112-9027-40be-f8c3-dd6175502386"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 51.6074\n",
            "Epoch 200 | Loss: 0.1032\n",
            "Epoch 400 | Loss: 0.0985\n",
            "Epoch 600 | Loss: 0.0980\n",
            "Epoch 800 | Loss: 0.0979\n",
            "Epoch 1000 | Loss: 0.0979\n",
            "Epoch 1200 | Loss: 0.0979\n",
            "Epoch 1400 | Loss: 0.0979\n",
            "Epoch 1600 | Loss: 0.0979\n",
            "Epoch 1800 | Loss: 0.0979\n",
            "Final Loss: 0.0979\n"
          ]
        }
      ]
    }
  ]
}