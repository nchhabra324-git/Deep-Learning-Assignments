{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MH5fECL3BAOD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and Inspect\n",
        "data = pd.read_csv(\"/content/multiple_linear_regression_dataset.csv\")\n",
        "\n",
        "print(data.head())\n",
        "print(\"\\nColumns:\", data.columns)\n",
        "print(\"Shape:\", data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mEtr6LhBLEx",
        "outputId": "11a665ed-35b2-475f-efe7-ff0590d0b9db"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age  experience  income\n",
            "0   25           1   30450\n",
            "1   30           3   35670\n",
            "2   47           2   31580\n",
            "3   32           5   40130\n",
            "4   43          10   47830\n",
            "\n",
            "Columns: Index(['age', 'experience', 'income'], dtype='object')\n",
            "Shape: (20, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Which columns are inputs -> experience and age.\n",
        "\n",
        "- Which column is the output -> income.\n",
        "\n",
        "- How many features does your model need to handle -> 2 features."
      ],
      "metadata": {
        "id": "q2KzmGTVBi35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Separate Inputs and Output\n",
        "X = data[[\"age\", \"experience\"]].values\n",
        "y = data[\"income\"].values"
      ],
      "metadata": {
        "id": "XzxaWL0fB1KR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- What is the shape of X -> (20, 2)\n",
        "\n",
        "- What is the shape of y -> (20, )\n",
        "\n",
        "- Why does X have 2 columns but y only one -> Because we are using 2 independent variables to predict 1 single dependent variable"
      ],
      "metadata": {
        "id": "6SOZyRWUDv9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Initialize the Model Parameters\n",
        "n = X.shape[1]\n",
        "w = np.zeros(n)\n",
        "b = 0.0"
      ],
      "metadata": {
        "id": "IyUUU0A6CFC5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Why do we need one weight per feature -> Because each feature affects the income differently. experience might be worth more money per unit than an age point. The model needs a specific multiplier for each to learn their distinct importance.\n",
        "\n",
        "- Why is bias separate -> Bias acts as the base income. It's the starting value when both age and experience are zero.\n",
        "\n",
        "- Would initializing with large values be risky -> Yes. Large initial weights would cause massive initial predictions, leading to gigantic errors. This can cause the gradients to explode, making the model instantly unstable."
      ],
      "metadata": {
        "id": "gaMVfOhCEOj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define the Forward Pass\n",
        "def predict(X, w, b):\n",
        "    return X.dot(w) + b"
      ],
      "metadata": {
        "id": "YK2avA53COSR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Why is there no activation function -> Activation functions are used to force outputs into specific categories or probabilities. Here, we want to predict an actual, unrestricted amount, so we just output the raw linear calculation.\n",
        "\n",
        "- What kind of values can y_pred take -> Any real number from -infinity to infinity.\n",
        "\n",
        "- How is this different from logistic regression -> Logistic regression squashes this exact same linear output into a probability between 0 and 1 using a sigmoid curve. Linear regression leaves it alone to predict continuous numbers."
      ],
      "metadata": {
        "id": "a-cZS7LyEs5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define the Loss Function (MSE)\n",
        "def mean_squared_error(y, y_pred):\n",
        "    return ((y_pred - y) ** 2).mean()"
      ],
      "metadata": {
        "id": "U_ll27wxCdVp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Why square the error -> It ensures all errors are positive so a prediction that is 1000 too high doesn't cancel out a prediction that is 1000 too low and it heavily penalizes massive mistakes.\n",
        "\n",
        "- What happens if one prediction is very wrong -> Because the error is squared, a huge mistake will result in a massive loss spike, forcing the model to aggressively correct itself on the next loop.\n",
        "\n",
        "- Why not just take absolute error -> Absolute error creates a V-shaped graph that isn't smooth at the very bottom , which can make calculating the gradient tricky for the math. MSE is a smooth bowl."
      ],
      "metadata": {
        "id": "pWEGV6BEFJYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Compute Gradients\n",
        "def compute_gradients(X, y, y_pred):\n",
        "    N = len(y)\n",
        "    dw = (2 / N) * X.T.dot(y_pred - y)\n",
        "    db = (2 / N) * (y_pred - y).sum()\n",
        "    return dw, db"
      ],
      "metadata": {
        "id": "2ftS5RIMCqN5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Why does X appear in dw but not in db -> Chain rule calculus! The weight w is directly multiplied by X in the forward pass equation (y = X*w + b). Therefore, the gradient of the weight scales with the size of the input X. The bias b is just a constant addition, so its derivative doesn't rely on X.\n",
        "\n",
        "- Why does the error term appear everywhere -> Because the error tells the model how much to change. If you are extremely wrong, you need a big update.\n",
        "\n",
        "- What happens if error is zero -> The gradients become zero. When dw and db are zero, the weights stop updating. The model has perfectly solved the dataset."
      ],
      "metadata": {
        "id": "QpGXeUdxFo74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Update Parameters\n",
        "def update_parameters(w, b, dw, db, lr):\n",
        "    w = w - lr * dw\n",
        "    b = b - lr * db\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "oUvIZsF8CxyR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Training Loop\n",
        "lr = 0.0001\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    y_pred = predict(X, w, b)\n",
        "    loss = mean_squared_error(y, y_pred)\n",
        "    dw, db = compute_gradients(X, y, y_pred)\n",
        "    w, b = update_parameters(w, b, dw, db, lr)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGuv1iKGC9Jx",
        "outputId": "0bbd4aab-d7b6-48fc-fb3e-c6ced8ba7a56"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1727049635.00\n",
            "Epoch 100, Loss: 66491868.55\n",
            "Epoch 200, Loss: 61752567.20\n",
            "Epoch 300, Loss: 58616531.08\n",
            "Epoch 400, Loss: 56528801.54\n",
            "Epoch 500, Loss: 55126542.03\n",
            "Epoch 600, Loss: 54172526.95\n",
            "Epoch 700, Loss: 53511656.14\n",
            "Epoch 800, Loss: 53042523.73\n",
            "Epoch 900, Loss: 52698829.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Does loss decrease over time -> Yes, it should steadily drop as the model learns.\n",
        "\n",
        "- What happens if it increases -> Your learning rate\n",
        " is too high. The model is taking steps that are too large and bouncing out of the error bowl.\n",
        "\n",
        "- How do learning rate and epochs interact -> If you use a very tiny learning rate, you are taking baby steps, meaning you will need a massive number of epochs to reach the goal. If you use a larger learning rate, you can get there in fewer epochs (but risk overshooting if it's too big)."
      ],
      "metadata": {
        "id": "lOUxDBOnF_OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Final Evaluation\n",
        "print(f\"Final Weights: {w}\")\n",
        "print(f\"Final Bias: {b}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnDY64JpDUN5",
        "outputId": "3cdea7e9-dc09-4637-edd8-f18e2c87e5a3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [ 764.75405919 1371.03430441]\n",
            "Final Bias: 321.73641174472493\n"
          ]
        }
      ]
    }
  ]
}